{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEDxGYC_yqTq"
   },
   "source": [
    "# SBU CSE 352 - HW 4 - Machine Learning From Scratch\n",
    "\n",
    "\n",
    "All student names in group: [Add your name]\n",
    "\n",
    "I understand that my submission needs to be my own group's work: [all group member's initials]\n",
    "\n",
    "I understand that ChatGPT / Copilot / other AI tools are not allowed: [all group member's initials]\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: 100\n",
    "\n",
    "1. Complete this notebook. Use the provided notebook cells and insert additional code and markdown cells as needed. Only use standard packages (numpy and built-in packages like random). Submit the completely rendered notebook as a HTML file.\n",
    "\n",
    "  **Important:** Do not use scikit-learn or other packages with ML built in. The point of this is to be a learning exercise. Using linear algebra from numpy is okay (things like matrix operations or pseudoinverse, for example, but not lstsq).\n",
    "\n",
    "2. Your notebook needs to be formatted professionally.\n",
    "    - Add additional markdown blocks for your description, comments in the code, add tables and use matplotlib to produce charts where appropriate\n",
    "    - Do not show debugging output or include an excessive amount of output.\n",
    "    - Check that your PDF file is readable. For example, long lines are cut off in the PDF file. You don't have control over page breaks, so do not worry about these.\n",
    "3. Document your code. Add a short discussion of how your implementation works and your design choices.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You will implement several machine learning algorithms and evaluate their accuracy. This will be done for a downscaled version of the MNIST digit recognition dataset.\n",
    "\n",
    "**Like in real life, some of the tasks you will be asked to do may not be possible, at least directly. In these cases, your job is to figure out why it won't work and either propose a fix (best), or provide a clear explanation why it won't work.**\n",
    "\n",
    "For example, if the problem says to do k-nearest neighbors with a dataset of a billion points, this could require too much time to do each classification so it's infeasible to evaluate its test accuracy. In this case, you could suggest randomly downsample the data to a more manageable size, which will speed things up by may lose some accuracy. In your answer, then, you should describe the problem and how you solved it and the trade-offs.\n",
    "\n",
    "# Data\n",
    "First the code below ensures you have access to the training data (a subset of the MNIST images), consisting of 100 handwritten images of each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTw87RlBzTOi",
    "outputId": "1844abe6-b3b4-408a-92a6-b36b0c86285f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerry\\Documents\\CS\\CSE 352\\CSE352\\HW4\\CS7320-AI\\ML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'CS7320-AI'...\n",
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# First download the repo and change the directory to be the one where the dependencies are.\n",
    "# You should only need to do this once per session. If you want to reset, do Runtime -> Disconnect and Delete Runtime\n",
    "# You can always do !pwd to see the current working directory and !ls to list current files.\n",
    "!git clone https://github.com/stanleybak/CS7320-AI.git\n",
    "%cd CS7320-AI/ML\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "ny3IAxVAyqTs",
    "outputId": "04ccd06a-6ac9-4935-e202-b70272e98355"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFQUlEQVR4nO3dTShsYRzH8SFSFjSzYyFKNixkKWHhJRNZKYWMZCmWytZa2VDKxl7NENKUlOxnYWs1FkPZUN7jrm/P/9xmjDP3/M75fpb/njvOvX3vqfNinqrv7+/vGCCm+n8fAPAThAtJhAtJhAtJhAtJhAtJhAtJhAtJhAtJNcUurKqq8vM4gFgsFosV+yCXMy4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kES4kFf0VTFHT0dFhzmtra51Zf3+/uXZ7e9uZfX19lXdgJcpkMs5senraXPv+/u734fwazriQRLiQRLiQRLiQRLiQVFXslqhh+GLnzs5Oc55KpZzZ1NSUuba62v2/3tzcbK61/s2CsAPt/v6+OV9dXXVmj4+PPh/N3/hiZ4Qa4UIS4UIS4UJSpC7ODg8PzXkymfTl5wX14szLwMCAM7u6uqroMXBxhlAjXEgiXEgiXEgiXEiK1Ivk2WzWnJdyV+H+/t6Z7e3tmWutx8OlvEje29trzq2r/6jhjAtJhAtJhAtJhAtJkXrkW1NjX4s2NTUV/RkfHx/OrFAo/PiY/qWhocGcX19fOzOvd4It6XTanM/MzDizt7e3oj/3N/DIF6FGuJBEuJBEuJBEuJAUqUe+n5+f5jyfz1f4SIozOjpqzuPxeFmfe3t7a84rfQehHJxxIYlwIYlwIYlwISlSj3yDzPqy5aWlJXNtue/jJhIJc17pr1uy8MgXoUa4kES4kES4kES4kBSpR76VZr2Yvba2Zq5tb293ZtbWVKXK5XLOzHoZXg1nXEgiXEgiXEgiXEiK1MVZa2urOZ+bm3NmQ0NDZf+8vr4+Z/YbX+xsPZr1uug7OTlxZi8vL2Ufw//GGReSCBeSCBeSCBeSCBeSQvsieVdXlzPz2i6qpaXFl2Pwa7uo4+NjZzY5OVn25wYBL5Ij1AgXkggXkggXkiL1yNfrAtOvC89yd93xMj4+7szGxsbMtaenp2X/vCDijAtJhAtJhAtJhAtJhAtJob2rYG2pNDg4aK6dnZ11ZmdnZ+ba19fXso7Ly+LiojNbXl725WeFAWdcSCJcSCJcSCJcSArt+7hqGhsbndnDw0PRf35iYsKcqz3y5X1chBrhQhLhQhLhQhLhQlJoH/mq8dq3FzbOuJBEuJBEuJBEuJAkdXFm7UIzMjJirj0/P3dmQfhC44WFBXO+tbVV4SPRxhkXkggXkggXkggXkggXkgJ5V8HaZikWi8XW19ed2fDwsLm2ra3NmeXz+fIOzEMikTDnyWTSmW1ubppr6+vri/551t0Rv377OKg440IS4UIS4UIS4UJSIH/LN5fLmXNrJx0vOzs7zuzp6emnh/RPXheIPT09zqyUXXcuLi7MufV3Ozg4KPpzg4zf8kWoES4kES4kES4kES4khfauQhBY/2Z3d3fm2qOjI2e2srJirg3z413uKiDUCBeSCBeSCBeSAnlx1t3dbc6tXWjm5+d9Ppq/3dzcOLPn52dz7eXlpTPb3d0111q7BEURF2cINcKFJMKFJMKFJMKFpEDeVfBSV1fnzFKplLl2Y2PDmcXjcXNtOp12Ztls1lybyWScWaFQMNeidNxVQKgRLiQRLiQRLiRJXZwh/Lg4Q6gRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiQVvZdvKdscAX7jjAtJhAtJhAtJhAtJhAtJhAtJhAtJhAtJhAtJfwBCXTEvdHk8ngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# if the below fails to open, then the data file is not in the current working directory (see above code block)\n",
    "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
    "  data = pickle.load(f)\n",
    "\n",
    "im3 = data['images'][300] # 100 images of each digit\n",
    "plt.figure(figsize=(2, 2))  # Adjust size as needed\n",
    "plt.imshow(im3, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kzts6NT5yqTt"
   },
   "source": [
    "# Downscaling Images\n",
    "\n",
    "MNIST images are originally 28x28. We will train our models not just on the original images, but also on downscaled images with the following sizes: 14x14, 7x7, 4x4, 2x2. The next code block shows one way to do downscaling. As you can tell from the output, we cannot expect our model's accuracy will be too high on lower resolution versions, although it's unclear how much better you can do than random chance, which should have a 10% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "2wTIXhvGyqTt",
    "outputId": "f27bb34d-8ee8-4ec8-8e5e-efc8e4388847",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to downscale an image to different sizes\n",
    "def downscale_image(image, downscaled_size):\n",
    "    block_size = 28 // downscaled_size\n",
    "    downscaled = np.zeros((downscaled_size, downscaled_size))\n",
    "    for i in range(downscaled_size):\n",
    "        for j in range(downscaled_size):\n",
    "            # Calculate the average for each block\n",
    "            block = image[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]\n",
    "            downscaled[i, j] = np.mean(block)\n",
    "    return downscaled\n",
    "\n",
    "# Load the dataset (assuming this file is in your working directory)\n",
    "with open('mini-mnist-1000.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "images = data['images']  # a list of 1000 numpy image matrices\n",
    "labels = data['labels']  # a list of 1000 integer labels\n",
    "\n",
    "# Select 3 \"random\" indices from the dataset\n",
    "random_indices = [300, 500, 200]\n",
    "\n",
    "# Downscale the images to multiple sizes and display them\n",
    "sizes = [28, 14, 7, 4, 2]\n",
    "for index in random_indices:\n",
    "    fig, axs = plt.subplots(1, len(sizes), figsize=(10, 2))\n",
    "    for ax, size in zip(axs, sizes):\n",
    "        downscaled_image = downscale_image(images[index], size)\n",
    "        ax.imshow(downscaled_image, cmap='gray', vmin=0, vmax=255)\n",
    "        ax.set_title(f'Size {size}x{size}')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXSTClGvQGpY",
    "outputId": "cda01e42-219d-431c-ab7d-5b3b1e52bf51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  13  25 100\n",
      "  122   7   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  33 151 208 252 252\n",
      "  252 146   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  40 152 244 252 253 224 211\n",
      "  252 232  40   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  15 152 239 252 252 252 216  31  37\n",
      "  252 252  60   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  96 252 252 252 252 217  29   0  37\n",
      "  252 252  60   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 181 252 252 220 167  30   0   0  77\n",
      "  252 252  60   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  26 128  58  22   0   0   0   0 100\n",
      "  252 252  60   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 157\n",
      "  252 252  60   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 110 121 122 121 202\n",
      "  252 194   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  10  53 179 253 253 255 253 253\n",
      "  228  35   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   5  54 227 252 243 228 170 242 252 252\n",
      "  231 117   6   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   6  78 252 252 125  59   0  18 208 252 252\n",
      "  252 252  87   7   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   5 135 252 252 180  16   0  21 203 253 247 129\n",
      "  173 252 252 184  66  49  49   0   0   0]\n",
      " [  0   0   0   0   0   3 136 252 241 106  17   0  53 200 252 216  65   0\n",
      "   14  72 163 241 252 252 223   0   0   0]\n",
      " [  0   0   0   0   0 105 252 242  88  18  73 170 244 252 126  29   0   0\n",
      "    0   0   0  89 180 180  37   0   0   0]\n",
      " [  0   0   0   0   0 231 252 245 205 216 252 252 252 124   3   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 207 252 252 252 252 178 116  36   4   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  13  93 143 121  23   6   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(f'{type(data)}')\n",
    "print(data['images'][200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YePL7s9NyqTw"
   },
   "source": [
    "---\n",
    "# Tasks\n",
    "\n",
    "Your data contains 100 images in each class. When training models, use 80% of training, 10% for validation and 10% for testing. Make sure the data is balanced in each class when splitting.\n",
    "\n",
    "---\n",
    "## Task 1: Linear Classifier [20 points]\n",
    "\n",
    "First, implement a linear classifier. The simplest way to do this is to adapt linear regression approaches that we learned about in class, where the output is a real number. For classification, we can let one category be an output of 1.0 and the other -1.0. Then, after the classifier is trained we can use the sign of the output to determine the predicted class.\n",
    "\n",
    "However, since in MNIST there are multiple classes (10 digits, not just 2), we need to adapt the approach further. We will try both of the following two popular strategies: One-vs-Rest (OvR) and One-vs-One (OvO).\n",
    "\n",
    "**One-vs-Rest (OvR)** is a strategy for using binary classification algorithms for multiclass problems. In this approach, a separate binary classifier is trained for each class, which predicts whether an instance belongs to that class or not, making it the 'one' against all other classes (the 'rest'). For a new input instance, compute the output of all classifiers. The predicted class is the one whose corresponding classifier gives the highest output value.\n",
    "\n",
    "**One-vs-One (OvO)** is another strategy where a binary classifier is trained for every pair of classes. If there are N classes, you will train N(N−1)/2 classifiers. For a new input, evaluate it using all N(N−1)/2​ classifiers. Count the number of times each class is predicted over all binary classifications. The class with the highest count is selected as the final prediction.\n",
    "\n",
    "### Report Results\n",
    "Report the test accuracy for OvR and OvO, for each of the input image sizes, 28x28, 14x14, 7x7, 4x4, 2x2. A table may be helpful. Also report any interesting observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ueSohSbyCljy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's do One-vs rest for each classs, we need to first split into training and test sets. \n",
    "\n",
    "# Let's use the convention that the first 80 for each class is training, next 10 for each class is \n",
    "\n",
    "training_data = []\n",
    "training_labels = []\n",
    "dev_data=[]\n",
    "dev_labels=[]\n",
    "test_data = []\n",
    "test_labels=[]\n",
    "for i in range(0,1000,100):\n",
    "    for x in range(i,i+80):\n",
    "        training_data.append(data['images'][x])\n",
    "        training_labels.append(data['labels'][x])\n",
    "    for x in range(i+80,i+90):\n",
    "        dev_data.append(data['images'][x])\n",
    "        dev_labels.append(data['labels'][x])\n",
    "    for x in range(i+90,i+100):\n",
    "        test_data.append(data['images'][x])\n",
    "        test_labels.append(data['labels'][x])\n",
    "\n",
    "#Downscale sizes\n",
    "\n",
    "sizes = [28,14,7,4,2]\n",
    "print(dev_labels[0:21])\n",
    "print()\n",
    "#print(training_labels[0:5])\n",
    "#print(dev_data[0:5])\n",
    "#print(dev_labels[0:5])\n",
    "#print(test_data[0:5])\n",
    "#print(test_labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets define the linear regression function, which will return a vector of weights\n",
    "\n",
    "def linearRegression(train_data, train_labels,num_steps = 1000, learning_rate=0.01):\n",
    "    weights = []\n",
    "    for _ in range(len(train_data[0])):\n",
    "        weights.append(random.random())\n",
    "    \n",
    "    weights = np.array(weights)\n",
    "    #print(f'initialized weights as {weights}')\n",
    "    train_data = np.array(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    \n",
    "    #Gradient of Loss function = 2X_t (Xw-y)\n",
    "    for _ in range(num_steps):\n",
    "        left = np.dot(2,train_data.T)\n",
    "        right = np.dot(train_data,weights)-train_labels\n",
    "        grad = np.dot(left,right)\n",
    "        weights = weights - np.dot(learning_rate,grad)\n",
    "    \n",
    "    return weights    \n",
    "\n",
    "def predict(models, data):\n",
    "    predictions = []\n",
    "    for vector in data:\n",
    "        curlabel = -1;\n",
    "        maxval = -10000\n",
    "        for label in range(9):\n",
    "            pred = np.dot(vector,models[label])\n",
    "            if pred>maxval:\n",
    "                maxval = pred\n",
    "                curlabel = label\n",
    "        predictions.append(label)\n",
    "    return predictions\n",
    "                \n",
    "                \n",
    "def accuracy(predictions,actual):\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i]==actual[i]:\n",
    "            correct+=1\n",
    "    return correct/len(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for dev set is 0.1\n",
      "accuracy for test set is 0.1\n",
      "accuracy for dev set is 0.1\n",
      "accuracy for test set is 0.1\n",
      "accuracy for dev set is 0.1\n",
      "accuracy for test set is 0.1\n",
      "accuracy for dev set is 0.1\n",
      "accuracy for test set is 0.1\n",
      "accuracy for dev set is 0.1\n",
      "accuracy for test set is 0.1\n"
     ]
    }
   ],
   "source": [
    "#Lets implment the One vs rest approach, where we train a model for each class, and each dataset is transformed to be 1 if it\n",
    "# is the target, and -1 if not.\n",
    "\n",
    "for size in sizes:\n",
    "    sized_train_data = [downscale_image(row,size) for row in training_data]\n",
    "    sized_dev_data = [downscale_image(row,size) for row in dev_data]\n",
    "    sized_test_data = [downscale_image(row,size) for row in test_data]\n",
    "    # Transform each entry from a size x size matrix to a 1 x size*size input vector for linear regression \n",
    "    \n",
    "    vectors = []\n",
    "    dev_vectors = []\n",
    "    test_vectors = []\n",
    "    \n",
    "    models = []\n",
    "    for vector in sized_train_data:\n",
    "        # We add a one term as a dummy bias term\n",
    "        vec = [1]\n",
    "        for i in range(len(vector)):\n",
    "            for j in range(len(vector[0])):\n",
    "                vec.append(vector[i][j])\n",
    "        vectors.append(vec)\n",
    "    \n",
    "    for vector in sized_dev_data:\n",
    "        # We add a one term as a dummy bias term\n",
    "        vec = [1]\n",
    "        for i in range(len(vector)):\n",
    "            for j in range(len(vector[0])):\n",
    "                vec.append(vector[i][j])\n",
    "        dev_vectors.append(vec)\n",
    "    \n",
    "    for vector in sized_test_data:\n",
    "        # We add a one term as a dummy bias term\n",
    "        vec = [1]\n",
    "        for i in range(len(vector)):\n",
    "            for j in range(len(vector[0])):\n",
    "                vec.append(vector[i][j])\n",
    "        test_vectors.append(vec)\n",
    "    for label in range(0,9):\n",
    "        #create new label classes:\n",
    "        sized_train_labels = [1 if label==x else -1 for x in training_labels]\n",
    "        weights = linearRegression(vectors,sized_train_labels)\n",
    "        models.append(weights)\n",
    "    \n",
    "    dev_predictions = predict(models,dev_vectors)\n",
    "    test_predictions = predict(models,test_vectors)\n",
    "    \n",
    "    dev_accuracy = accuracy(dev_predictions,dev_labels)\n",
    "    test_accuracy = accuracy(test_predictions, test_labels)\n",
    "    \n",
    "    print(f'accuracy for dev set is {dev_accuracy}')\n",
    "    print(f'accuracy for test set is {test_accuracy}')\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_6z07BdyqTw"
   },
   "source": [
    "---\n",
    "## Task 2: Data Augmentation [20 points]\n",
    "\n",
    "Your boss was unhappy with the test accuracy, especially of your 2x2 image classifier, and has made some suggestions. The problem, according to your boss, is that there is not enough data in each input $x$. You are told to augment the data with derived features in order to help the classifier.\n",
    "\n",
    "Specifically, given an input $x$, create additional attributes by computing all of the data up to powers of two. For example, in the 2x2 case your example $x$ consists of four pixel values $x_0$, $x_1$, $x_2$, and $x_3$. Your new input data would have:\n",
    "\n",
    "* all power of zero: 1 (constant)\n",
    "* all powers of one: $x_0$, $x_1$, $x_2$, $x_3$\n",
    "* all powers of two:\n",
    "\n",
    "  $x_0^2$, $x_0 x_1$, $x_0 x_2$, $x_0 x_3$,\n",
    "  \n",
    "  $x_1^2$, $x_1 x_2$, $ x_1 x_3$,\n",
    "  \n",
    "  $x_2^2$, $x_2 x_3$,\n",
    "  \n",
    "  $x_3^2$\n",
    "\n",
    "The data would have 15 values, which has the potential to learn nonlinear relationships between the original inputs, which was not possible before.\n",
    "\n",
    "### Report Results\n",
    "\n",
    "Report the test accuracy for OvR only, with the data augmentation approach, for each of the input image sizes, 28x28, 14x14, 7x7, 4x4, 2x2 (again, perhaps incorporating a table). Report any interesting results or observations.\n",
    "\n",
    "Also, explain to your boss what the danger is of looking at a model's final test accuracy and then suggesting changes to improve it. What should be done instead, if you know you will consider different types of models or hyperparameters in the same model class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCvHIrBwyqTw"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xz7hMCAkyqTw"
   },
   "source": [
    "---\n",
    "## Task 3: k-Nearest Neighbors Classifier [20 points]\n",
    "\n",
    "Your boss is still unhappy with the results (and still ignoring your advice about not using test data accuracy for model decisions).\n",
    "\n",
    "Next, you are to use the k-nearest neighbors approach to build a classifier for our data. Since we have multiple classes, the one that gets selected can be based on a plurality vote of the $k$ closest samples (whichever category is most frequent). If there are ties, select the class based on the sum of the distances from the test point. For example, if $k=5$, and the closest 5 samples have two pictures that are from category \"1\" and two pictures that are from category \"7\", then you choose the output by computing the sum of the distance from the test point and the two \"5\" samples, as well as the sum of distances from the test point to the two \"7\" samples, and then outputting the class with the smaller total distance.\n",
    "\n",
    "### Report Results\n",
    "\n",
    "For each image size, exhaustively explore different values of $k$ up to 50. Report the best test accuracy. Report the average time taken to do a lookup with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7pGs5acyqTw"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyN1oAczyqTw"
   },
   "source": [
    "---\n",
    "## Task 4: Neural Networks [40 Points]\n",
    "\n",
    "Next, your boss wants you to try neural networks. Rather than using a library for everything, you will **only** use `pytorch` to perform backpropagation and compute gradients. You can write your own neural network class if desired, don't use anything from `pytorch` for that.\n",
    "\n",
    "\n",
    "An example network and how to compute gradients with pytorch is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVYHnm2fyqTx",
    "outputId": "9fbbd6d3-1316-4fcc-b43d-8218569ba8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Output: tensor([0.5348, 0.2167, 0.2485], grad_fn=<SoftmaxBackward0>)\n",
      "Desired Output: tensor([1., 0., 0.])\n",
      "Initial loss: 0.625852644443512\n",
      "Gradient for weights matrix W1: tensor([[-2.9431e-03, -5.8862e-03, -8.8293e-03],\n",
      "        [ 4.1993e-03,  8.3986e-03,  1.2598e-02],\n",
      "        [-3.0524e-06, -6.1048e-06, -9.1572e-06]])\n",
      "New loss after updating weights and biases: 0.6079817414283752\n"
     ]
    }
   ],
   "source": [
    "# Example of using pytorch to compute gradients and updates weights and biases\n",
    "#\n",
    "# The network consists of:\n",
    "# 1. An input layer with 3 features.\n",
    "# 2. A first hidden layer with 3 neurons. Each neuron in this layer performs a linear transformation\n",
    "#    on the input data using a weight matrix (W1) and a bias vector (b1). This is followed by a sigmoid\n",
    "#    activation function.\n",
    "# 3. A second hidden layer, also with 3 neurons, which processes the output of the first layer. Similar\n",
    "#    to the first layer, it uses a weight matrix (W2) and a bias vector (b2) for linear transformation,\n",
    "#    followed by a softmax activation function. The softmax activation is used here to normalize the\n",
    "#    output of the second layer into a probability distribution over the three classes. This is particularly\n",
    "#    useful for multi-class classification problems.\n",
    "# 4. The network uses cross-entropy as the loss function, which is a common choice for classification tasks\n",
    "#    involving softmax outputs. This loss function compares the predicted probability distribution with the\n",
    "#    true distribution (one-hot encoded) and penalizes the predictions that diverge from the actual labels.\n",
    "#\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Initialize input, weights, and biases\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "W1 = torch.tensor([[0.1, 0.2, 0.5],\n",
    "                  [-0.1, -0.5, -1.1],\n",
    "                  [0, 7.5, -1.1]], requires_grad=True)\n",
    "b1 = torch.tensor([0.0, 0.0, 0.0], requires_grad=True)\n",
    "\n",
    "W2 = torch.tensor([[0.1, -0.3, 0.4],\n",
    "                  [0.2, 0.4, -0.6],\n",
    "                  [-0.1, 0.5, -0.2]], requires_grad=True)\n",
    "b2 = torch.tensor([0.0, 0.0, 0.0], requires_grad=True)\n",
    "\n",
    "# Target output\n",
    "y_true = torch.tensor([1.0, 0.0, 0.0])\n",
    "\n",
    "# Forward pass through first layer\n",
    "z1 = torch.matmul(W1, x) + b1\n",
    "a1 = torch.sigmoid(z1)  # Sigmoid activation\n",
    "\n",
    "# Forward pass through second layer\n",
    "z2 = torch.matmul(W2, a1) + b2\n",
    "a2 = torch.softmax(z2, dim=0)  # Softmax activation\n",
    "\n",
    "print(\"Initial Output:\", a2)\n",
    "print(\"Desired Output:\", y_true)\n",
    "\n",
    "# Compute loss (Cross-entropy): https://en.wikipedia.org/wiki/Cross-entropy\n",
    "loss = -torch.sum(y_true * torch.log(a2))\n",
    "print(\"Initial loss:\", loss.item())\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# you can print out gradient for each element now\n",
    "print(\"Gradient for weights matrix W1:\", W1.grad)\n",
    "\n",
    "# Update weights and biases based on gradient (should reduce loss)\n",
    "learning_rate = 0.02\n",
    "\n",
    "# the no_grad() environment is needed to indicate that the computation should not\n",
    "# be part of the gradient computation\n",
    "with torch.no_grad():\n",
    "    W1 -= learning_rate * W1.grad\n",
    "    b1 -= learning_rate * b1.grad\n",
    "    W2 -= learning_rate * W2.grad\n",
    "    b2 -= learning_rate * b2.grad\n",
    "\n",
    "# After the update, clear the gradients (in case we want to compute them again later)\n",
    "W1.grad.zero_()\n",
    "b1.grad.zero_()\n",
    "W2.grad.zero_()\n",
    "b2.grad.zero_()\n",
    "\n",
    "# Forward pass with updated weights and biases\n",
    "z1 = torch.matmul(W1, x) + b1\n",
    "a1 = torch.sigmoid(z1)  # Sigmoid activation\n",
    "z2 = torch.matmul(W2, a1) + b2\n",
    "a2 = torch.softmax(z2, dim=0)  # Softmax activation\n",
    "\n",
    "# Compute new loss\n",
    "new_loss = -torch.sum(y_true * torch.log(a2))\n",
    "print(\"New loss after updating weights and biases:\", new_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hddWtjp7bHvD"
   },
   "source": [
    "The code above updates the parameters based on a single piece of data, but often multiple inputs are used and their gradient is averaged when updating a model.\n",
    "\n",
    "Your task is to write the training code for the different neural network architectures proposed and report accuracy. Start with all random parameters between -1 and 1. Training should stop when the accuracy, as measured on the validation data, no longer appears to be improving. You can plot the validation data accuracy over time to ensure this looks correct. If this takes too long but it appears the model is still improving in accuracy, consider increasing the learning rate (start with 0.02 as in the example).\n",
    "\n",
    "For the gradient, you are to compute the gradient over the full set of training data, and then average them together before you update. Then, repeat with mini-batches of size 100, with 10 random samples from each class. This should update the model weights faster, but may require more updates to get the accuracy down.\n",
    "\n",
    "### Report Results\n",
    "\n",
    "Provide at least one plot of your validation data accuracy going down over time as training progresses. What was the condition you decided to use to detect if training should stop? How many updates were needed in the case of your plot?\n",
    "\n",
    "\n",
    "Create a table where each row corresponds to one model and training method (mini-batch or full). Use the 7x7 version of the data (49-dimensional inputs). You are to explore the following models: the number of hidden layers can be varied between 2 and 4. Each layer's size can be 16, 32, or 64 neurons (all hidden layers have the same number of neurons). Explore three different activation functions for the network, ReLU (`torch.relu`), arctan (`torch.atan`), and sigmoid (`torch.sigmoid`). After the final layer, use a softmax rather than the normal network activation function, to ensure all outputs are between 0 and 1. There should be 10 outputs, one for each class in the MNIST data.\n",
    "\n",
    "In the table, report the architecture, training time, number of model updates and test accuracy. What is the best architecture? Did mini-batches help with anything? Report any other interesting observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxJf4LR8f54p"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
